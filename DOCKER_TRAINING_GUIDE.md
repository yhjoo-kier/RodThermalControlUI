# Docker í™˜ê²½ì—ì„œ RL í•™ìŠµ ì‹¤í–‰ ê°€ì´ë“œ

Docker Composeë¡œ ì‹¤í–‰ ì¤‘ì¸ í™˜ê²½ì—ì„œ RL í•™ìŠµì„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

## ë°©ë²• 1: ì›¹ UI ì‚¬ìš© (ê°€ì¥ ê°„ë‹¨) â­

ì›¹ UIì—ì„œ ì§ì ‘ í•™ìŠµì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

1. ë¸Œë¼ìš°ì €ì—ì„œ `http://localhost:5173` ì ‘ì†
2. ìƒë‹¨ ë„¤ë¹„ê²Œì´ì…˜ ë°”ì—ì„œ **"ğŸ“ Train RL Model"** ë²„íŠ¼ í´ë¦­
3. í•™ìŠµ ëª¨ë‹¬ì—ì„œ ì„¤ì • ì„ íƒ:
   - **Quick Presets** ë²„íŠ¼ìœ¼ë¡œ ë¹ ë¥¸ ì„¤ì •
   - ë˜ëŠ” ê°œë³„ íŒŒë¼ë¯¸í„° ì¡°ì •
4. ëª¨ë‹¬ í•˜ë‹¨ì˜ ë²„íŠ¼ í´ë¦­í•˜ì—¬ í•™ìŠµ ì‹œì‘
5. ì‹¤ì‹œê°„ìœ¼ë¡œ ì§„í–‰ìƒí™© ëª¨ë‹ˆí„°ë§

**ì¥ì :**
- GUIë¡œ í¸ë¦¬í•˜ê²Œ ì„¤ì • ê°€ëŠ¥
- ì‹¤ì‹œê°„ ì§„í–‰ìƒí™© í™•ì¸
- ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ë˜ì–´ ë‹¤ë¥¸ ì‘ì—… ê°€ëŠ¥

## ë°©ë²• 2: Docker Execë¡œ CLI ëª…ë ¹ ì‹¤í–‰

ì‹¤í–‰ ì¤‘ì¸ backend ì»¨í…Œì´ë„ˆì—ì„œ ì§ì ‘ Python ëª…ë ¹ ì‹¤í–‰:

### 2-1. ê¸°ë³¸ ì‚¬ìš©ë²•

```bash
# 1. Docker ì»¨í…Œì´ë„ˆê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
docker-compose ps

# 2. backend ì»¨í…Œì´ë„ˆì—ì„œ í•™ìŠµ ì‹¤í–‰
docker-compose exec backend python backend/train_rl.py --quick
```

### 2-2. ë‹¤ì–‘í•œ í•™ìŠµ ì˜µì…˜

```bash
# Quick Test (20k timesteps, ~2-5ë¶„)
docker-compose exec backend python backend/train_rl.py --quick --reward shaped

# Standard Training (500k timesteps, ~30-60ë¶„)
docker-compose exec backend python backend/train_rl.py --standard --reward shaped

# GPU Training (2M timesteps) - GPU ì„¤ì • í•„ìš”
docker-compose exec backend python backend/train_rl.py --gpu --reward shaped --device cuda

# Custom Configuration
docker-compose exec backend python backend/train_rl.py \
  --timesteps 1000000 \
  --envs 12 \
  --reward shaped \
  --lr 0.0003 \
  --device auto
```

### 2-3. ì‹¤ì‹œê°„ ë¡œê·¸ í™•ì¸

ë³„ë„ í„°ë¯¸ë„ì—ì„œ ë¡œê·¸ í™•ì¸:

```bash
# í„°ë¯¸ë„ 1: í•™ìŠµ ì‹¤í–‰
docker-compose exec backend python backend/train_rl.py --standard

# í„°ë¯¸ë„ 2: ë¡œê·¸ ìŠ¤íŠ¸ë¦¬ë°
docker-compose logs -f backend
```

## ë°©ë²• 3: ì¼íšŒì„± ì»¨í…Œì´ë„ˆë¡œ ì‹¤í–‰

í•™ìŠµë§Œì„ ìœ„í•œ ë³„ë„ ì»¨í…Œì´ë„ˆë¥¼ ì‹¤í–‰:

```bash
# ì¼íšŒì„± ì»¨í…Œì´ë„ˆë¡œ í•™ìŠµ ì‹¤í–‰
docker-compose run --rm backend python backend/train_rl.py --standard --reward shaped

# ë°±ê·¸ë¼ìš´ë“œë¡œ ì‹¤í–‰í•˜ê³  ë¡œê·¸ íŒŒì¼ ì €ì¥
docker-compose run -d --rm backend python backend/train_rl.py --gpu > training.log 2>&1
```

**ì¥ì :**
- ë©”ì¸ ì„œë¹„ìŠ¤ì™€ ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰
- í•™ìŠµ ì™„ë£Œ í›„ ìë™ìœ¼ë¡œ ì»¨í…Œì´ë„ˆ ì œê±° (--rm)

## ë°©ë²• 4: GPU ì§€ì› ì„¤ì • (GPUê°€ ìˆëŠ” ê²½ìš°)

GPUë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ docker-compose.ymlì„ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤.

### 4-1. docker-compose.yml ìˆ˜ì •

```yaml
version: '3.8'

services:
  backend:
    build: ./backend
    ports:
      - "8000:8000"
    volumes:
      - ./backend:/app/backend
      - ./ppo_thermal_rod.zip:/app/ppo_thermal_rod.zip
    environment:
      - PYTHONUNBUFFERED=1
    # GPU ì„¤ì • ì¶”ê°€
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  frontend:
    image: node:18-alpine
    working_dir: /app
    volumes:
      - ./frontend:/app
    ports:
      - "5173:5173"
    command: sh -c "npm install && npm run dev -- --host"
```

### 4-2. nvidia-docker í™•ì¸

```bash
# NVIDIA Docker Runtime í™•ì¸
docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi

# ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ë‹¤ë©´
# https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html
```

### 4-3. GPUë¡œ í•™ìŠµ ì‹¤í–‰

```bash
# docker-compose.yml ìˆ˜ì • í›„
docker-compose down
docker-compose up -d --build

# GPU í•™ìŠµ ì‹¤í–‰
docker-compose exec backend python backend/train_rl.py --gpu --device cuda
```

## í•™ìŠµ ëª¨ë¸ ê´€ë¦¬

### ê¸°ì¡´ ëª¨ë¸ ì‚­ì œ (ìƒˆë¡œ í•™ìŠµí•  ë•Œ)

```bash
# í˜¸ìŠ¤íŠ¸ì—ì„œ ì‚­ì œ
rm ppo_thermal_rod.zip

# ë˜ëŠ” ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ ì‚­ì œ
docker-compose exec backend rm /app/ppo_thermal_rod.zip
```

### í•™ìŠµëœ ëª¨ë¸ í™•ì¸

```bash
# ëª¨ë¸ íŒŒì¼ í™•ì¸
ls -lh ppo_thermal_rod.zip

# ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ í™•ì¸
docker-compose exec backend ls -lh /app/ppo_thermal_rod.zip
```

### ì²´í¬í¬ì¸íŠ¸ í™•ì¸

```bash
# í•™ìŠµ ì¤‘ ì €ì¥ë˜ëŠ” ì²´í¬í¬ì¸íŠ¸
docker-compose exec backend ls -lh /app/checkpoints/

# í˜¸ìŠ¤íŠ¸ì—ì„œë„ í™•ì¸ ê°€ëŠ¥ (ë³¼ë¥¨ ë§ˆìš´íŠ¸ ì‹œ)
ls -lh ./checkpoints/
```

## í•™ìŠµ ëª¨ë‹ˆí„°ë§

### ë°©ë²• 1: ì›¹ UI
- `http://localhost:5173`ì—ì„œ ì‹¤ì‹œê°„ ì§„í–‰ìƒí™© í™•ì¸

### ë°©ë²• 2: Docker ë¡œê·¸
```bash
# ì‹¤ì‹œê°„ ë¡œê·¸ ìŠ¤íŠ¸ë¦¬ë°
docker-compose logs -f backend

# ìµœê·¼ 100ì¤„
docker-compose logs --tail=100 backend
```

### ë°©ë²• 3: TensorBoard (ì„ íƒì‚¬í•­)
```bash
# TensorBoard ì‹¤í–‰ (í¬íŠ¸ 6006)
docker-compose exec backend tensorboard --logdir=/app/logs --host=0.0.0.0 --port=6006

# ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:6006 ì ‘ì†
```

docker-compose.ymlì— TensorBoard ì„œë¹„ìŠ¤ ì¶”ê°€:
```yaml
  tensorboard:
    image: tensorflow/tensorflow:latest
    ports:
      - "6006:6006"
    volumes:
      - ./logs:/logs
    command: tensorboard --logdir=/logs --host=0.0.0.0
```

## í•™ìŠµ ì¤‘ë‹¨ ë° ì¬ê°œ

### í•™ìŠµ ì¤‘ë‹¨

**ì›¹ UIì—ì„œ:**
- í•™ìŠµ ëª¨ë‹¬ì˜ "Stop Training" ë²„íŠ¼ í´ë¦­

**CLIì—ì„œ:**
- `Ctrl+C`ë¡œ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
- ìë™ìœ¼ë¡œ `ppo_thermal_rod_interrupted.zip` ì €ì¥ë¨

### í•™ìŠµ ì¬ê°œ

ì´ì „ì— ì €ì¥ëœ ëª¨ë¸ì´ ìˆìœ¼ë©´ ìë™ìœ¼ë¡œ ë¡œë“œí•˜ì—¬ ê³„ì† í•™ìŠµ:

```bash
# ëª¨ë¸ì´ ìˆìœ¼ë©´ ìë™ìœ¼ë¡œ ê³„ì† í•™ìŠµ
docker-compose exec backend python backend/train_rl.py --standard
```

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### GPU ê´€ë ¨ ì˜¤ë¥˜
```bash
# CUDAë¥¼ ì°¾ì„ ìˆ˜ ì—†ë‹¤ëŠ” ì˜¤ë¥˜ ì‹œ
docker-compose exec backend python -c "import torch; print(torch.cuda.is_available())"

# Falseê°€ ë‚˜ì˜¤ë©´ CPUë¡œ í•™ìŠµ
docker-compose exec backend python backend/train_rl.py --standard --device cpu
```

### ë©”ëª¨ë¦¬ ë¶€ì¡±
```bash
# ë³‘ë ¬ í™˜ê²½ ìˆ˜ ì¤„ì´ê¸°
docker-compose exec backend python backend/train_rl.py --timesteps 500000 --envs 2
```

### ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘
```bash
# ë³€ê²½ì‚¬í•­ ì ìš© í›„ ì¬ì‹œì‘
docker-compose restart backend

# ì™„ì „íˆ ì¬ë¹Œë“œ
docker-compose down
docker-compose up -d --build
```

## ê¶Œì¥ ì›Œí¬í”Œë¡œìš°

### CPU í™˜ê²½ (ì¼ë°˜ ê°œë°œ)
```bash
# 1. ê¸°ì¡´ ëª¨ë¸ ì‚­ì œ
rm ppo_thermal_rod.zip

# 2. ì»¨í…Œì´ë„ˆ ì‹¤í–‰
docker-compose up -d

# 3-1. ì›¹ UIì—ì„œ í•™ìŠµ (ì¶”ì²œ)
#      http://localhost:5173 ì ‘ì† â†’ Train RL Model ë²„íŠ¼

# 3-2. ë˜ëŠ” CLIë¡œ í•™ìŠµ
docker-compose exec backend python backend/train_rl.py --standard --reward shaped

# 4. í•™ìŠµ ì™„ë£Œ í›„ ì‹œë®¬ë ˆì´ì…˜ì—ì„œ í™•ì¸
#    RL Agentê°€ ì ê·¹ì ìœ¼ë¡œ ì œì–´í•˜ëŠ”ì§€ í™•ì¸
```

### GPU í™˜ê²½ (ê³ ì„±ëŠ¥ í•™ìŠµ)
```bash
# 1. GPU ì„¤ì • í™•ì¸
docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi

# 2. docker-compose.ymlì— GPU ì„¤ì • ì¶”ê°€ (ìœ„ ì°¸ì¡°)

# 3. ì¬ë¹Œë“œ ë° ì‹¤í–‰
docker-compose down
docker-compose up -d --build

# 4. GPU í•™ìŠµ ì‹¤í–‰
docker-compose exec backend python backend/train_rl.py --gpu --reward shaped --device cuda

# 5. ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
docker-compose logs -f backend
```

## í•™ìŠµ ì‹œê°„ ì°¸ê³ 

| ì„¤ì • | Timesteps | CPU ì‹œê°„ | GPU ì‹œê°„ |
|------|-----------|----------|----------|
| Quick | 20,000 | ~2-5ë¶„ | ~1-2ë¶„ |
| Standard | 500,000 | ~30-60ë¶„ | ~10-20ë¶„ |
| GPU | 2,000,000 | ~2-4ì‹œê°„ | ~30-60ë¶„ |
| Intensive | 10,000,000 | ~10-20ì‹œê°„ | ~2-4ì‹œê°„ |

*ì‹¤ì œ ì‹œê°„ì€ í•˜ë“œì›¨ì–´ ì‚¬ì–‘ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤*

## ì¶”ê°€ íŒ

1. **ë¹ ë¥¸ í…ŒìŠ¤íŠ¸**: ë¨¼ì € `--quick` ì˜µì…˜ìœ¼ë¡œ ì„¤ì •ì´ ì •ìƒ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸
2. **ì ì§„ì  í•™ìŠµ**: 100k â†’ 500k â†’ 2M ìˆœìœ¼ë¡œ ì ì§„ì ìœ¼ë¡œ í•™ìŠµëŸ‰ ì¦ê°€
3. **ë¡œê·¸ ì €ì¥**: ê¸´ í•™ìŠµ ì‹œ ë¡œê·¸ íŒŒì¼ë¡œ ì €ì¥í•˜ì—¬ ë‚˜ì¤‘ì— ë¶„ì„
4. **ëª¨ë¸ ë°±ì—…**: ì¢‹ì€ ì„±ëŠ¥ì˜ ëª¨ë¸ì€ ë³„ë„ íŒŒì¼ëª…ìœ¼ë¡œ ë°±ì—…

```bash
# ëª¨ë¸ ë°±ì—…
cp ppo_thermal_rod.zip ppo_thermal_rod_backup_$(date +%Y%m%d_%H%M%S).zip
```
